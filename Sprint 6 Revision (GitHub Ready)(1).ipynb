{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:solid blue 2px; padding: 20px\">\n",
    "  \n",
    "**Hello Byron.**\n",
    "\n",
    "I'm happy to review your project today. \n",
    "\n",
    "You will find my comments in coloured cells marked as 'Reviewer's comment'. The cell colour will vary based on the contents - I am explaining it further below. \n",
    "\n",
    "**Note:** Please do not remove or change my comments - they will help me in my future reviews and will make the process smoother for both of us. \n",
    "\n",
    "<div class=\"alert alert-success\"; style=\"border-left: 7px solid green\">\n",
    "<b>✅ Reviewer's comment</b> \n",
    "    \n",
    "Such comment will mark efficient solutions and good ideas that can be used in other projects.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-warning\"; style=\"border-left: 7px solid gold\">\n",
    "<b>⚠️ Reviewer's comment</b> \n",
    "    \n",
    "The parts marked with yellow comments indicate that there is room for optimisation. Though the correction is not necessary it is good if you implement it.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-danger\"; style=\"border-left: 7px solid red\">\n",
    "<b>⛔️ Reviewer's comment</b> \n",
    "    \n",
    "If you see such a comment, it means that there is a problem that needs to be fixed. Please note that I won't be able to accept your project until the issue is resolved.\n",
    "</div>\n",
    "\n",
    "You are also very welcome to leave your comments / describe the corrections you've done / ask me questions, marking them with a different colour. You can use the example below: \n",
    "\n",
    "<div class=\"alert alert-info\"; style=\"border-left: 7px solid blue\">\n",
    "<b>Student's comment</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:solid green 2px; padding: 20px\">\n",
    "    \n",
    "<div class=\"alert alert-success\">\n",
    "<b>Review summary</b> \n",
    "    \n",
    "Byron, thanks for submitting the project. You've done an outstanding job and I enjoyed reviewing it.\n",
    "    \n",
    "- You completed all the tasks.\n",
    "- You did EDA and conducted the statistical test flawlessly.\n",
    "    \n",
    "I've got no critical comments, just a few recommendations. \n",
    "    \n",
    "Keep up the good work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint 6 Project, Title: Data Collection And Storage (SQL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary: \n",
    "This project consists of exploratory data analyses  and hypothesis testing of information from Zuber, a new ride sharing company in Chicago.  The main objective of this project is to understand passenger preferences and the impact of external factor on rides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"; style=\"border-left: 7px solid green\">\n",
    "<b>✅ Reviewer's comment, v. 1</b> \n",
    "    \n",
    "Well done that you start with the title and introduction.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the Neccessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the necessary libraries.\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns \n",
    "\n",
    "from scipy import stats as st\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading the Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the website address for the project as a string in order to parse the data from the website.\n",
    "URL = 'https://practicum-content.s3.us-west-1.amazonaws.com/data-analyst-eng/moved_chicago_weather_2017.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the get() function from the requests library in order to send the requests for data from\n",
    "#the web server from the website provided in the URL variable.\n",
    "req = requests.get(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the BeautifulSoup() function in order to prase the data from the req variable into lxml text\n",
    "#format.\n",
    "soup = BeautifulSoup(req.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use find() function in order to find the first occurence of the value \"table\".  Set the \n",
    "#attributes variable to a dictionary titled, \"id\": \"weather records\".\n",
    "table = soup.find('table', attrs={\"id\": \"weather_records\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the text function in order to rerun a text output from the headers in the parsed data.\n",
    "#Use the find_all() function to find all the headers sections ('th') in the parsed data (lxml) in \n",
    "#order to retrieve them as input for the string.\n",
    "headers = [header.text for header in table.find_all('th')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an empty list to hold all the table rows.\n",
    "rows = []\n",
    "\n",
    "#Create a for loop that loops through every <tr> (table row) element in the HTML table.\n",
    "for row in table.find_all('tr'):\n",
    "    #Extract all <td> (data cell elements from the row.\n",
    "    cells = row.find_all('td')\n",
    "    #Only proccess rows that contain data cells.  This if statement should skip header or empty rows.\n",
    "    if len(cells) > 0:\n",
    "        #Extract the text from each cell and append the test to the 'rows' list.\n",
    "        rows.append([cell.text for cell in cells])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date and time</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-11-01 00:00:00</td>\n",
       "      <td>276.150</td>\n",
       "      <td>broken clouds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-11-01 01:00:00</td>\n",
       "      <td>275.700</td>\n",
       "      <td>scattered clouds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-11-01 02:00:00</td>\n",
       "      <td>275.610</td>\n",
       "      <td>overcast clouds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-11-01 03:00:00</td>\n",
       "      <td>275.350</td>\n",
       "      <td>broken clouds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-11-01 04:00:00</td>\n",
       "      <td>275.240</td>\n",
       "      <td>broken clouds</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date and time Temperature       Description\n",
       "0  2017-11-01 00:00:00     276.150     broken clouds\n",
       "1  2017-11-01 01:00:00     275.700  scattered clouds\n",
       "2  2017-11-01 02:00:00     275.610   overcast clouds\n",
       "3  2017-11-01 03:00:00     275.350     broken clouds\n",
       "4  2017-11-01 04:00:00     275.240     broken clouds"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use the pd.Dataframe() function in order to create a dataframe using the data in the previously created rows \n",
    "#list as the input for the rows and use the headers variables as the input for the columns.\n",
    "weather_records = pd.DataFrame(rows, columns=headers)\n",
    "\n",
    "#Use the head() method to print the first five rows of the dataframe.\n",
    "weather_records.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_and_time</th>\n",
       "      <th>temperature</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-11-01 00:00:00</td>\n",
       "      <td>276.150</td>\n",
       "      <td>broken clouds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-11-01 01:00:00</td>\n",
       "      <td>275.700</td>\n",
       "      <td>scattered clouds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-11-01 02:00:00</td>\n",
       "      <td>275.610</td>\n",
       "      <td>overcast clouds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-11-01 03:00:00</td>\n",
       "      <td>275.350</td>\n",
       "      <td>broken clouds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-11-01 04:00:00</td>\n",
       "      <td>275.240</td>\n",
       "      <td>broken clouds</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date_and_time temperature       description\n",
       "0  2017-11-01 00:00:00     276.150     broken clouds\n",
       "1  2017-11-01 01:00:00     275.700  scattered clouds\n",
       "2  2017-11-01 02:00:00     275.610   overcast clouds\n",
       "3  2017-11-01 03:00:00     275.350     broken clouds\n",
       "4  2017-11-01 04:00:00     275.240     broken clouds"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use the rename() method in order to rename the columns in the weather_records dataframe appropriately.\n",
    "weather_records = weather_records.rename(columns={'Date and time' : \n",
    "                                                  'date_and_time', \n",
    "                                                  'Temperature': 'temperature',\n",
    "                                                  'Description': 'description'})\n",
    "\n",
    "#Use the head() method in order print the first five rows of the dataframe.\n",
    "weather_records.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"; style=\"border-left: 7px solid green\">\n",
    "<b>✅ Reviewer's comment, v. 1</b> \n",
    "    \n",
    "For this project I'm starting my review here.</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'moved_project_sql_result_01 (2).csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 4\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 4\u001b[0m     company_trips \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/datasets/project_sql_result_01.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1736\u001b[0m     f,\n\u001b[0;32m   1737\u001b[0m     mode,\n\u001b[0;32m   1738\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1739\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1740\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1741\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1742\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1743\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1744\u001b[0m )\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m     handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    857\u001b[0m         handle,\n\u001b[0;32m    858\u001b[0m         ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    859\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    860\u001b[0m         errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    861\u001b[0m         newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    862\u001b[0m     )\n\u001b[0;32m    863\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m     \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/datasets/project_sql_result_01.csv'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m     company_trips \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/datasets/project_sql_result_01.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m----> 6\u001b[0m     company_trips \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmoved_project_sql_result_01 (2).csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#Use the head() method in order to print the first five rows of the company_trips dataframe.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m company_trips\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1736\u001b[0m     f,\n\u001b[0;32m   1737\u001b[0m     mode,\n\u001b[0;32m   1738\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1739\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1740\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1741\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1742\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1743\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1744\u001b[0m )\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    859\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    860\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    861\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'moved_project_sql_result_01 (2).csv'"
     ]
    }
   ],
   "source": [
    "#Create a try and except clause that reads/uploads the dataframe in a local and non-local\n",
    "#computer.\n",
    "try:\n",
    "    company_trips = pd.read_csv('/datasets/project_sql_result_01.csv')\n",
    "except:\n",
    "    company_trips = pd.read_csv('moved_project_sql_result_01 (2).csv')\n",
    "\n",
    "#Use the head() method in order to print the first five rows of the company_trips dataframe.\n",
    "company_trips.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"; style=\"border-left: 7px solid gold\">\n",
    "<b>⚠️ Reviewer's comment, v. 1</b> \n",
    "\n",
    "\n",
    "    \n",
    "Please note that you don't need to describe the code so thorougly. It is advisable to describe more your observations / takeaways. </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I recently edited this notebook and I fixed this issue.  Please disregard this reviewer's note."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a try and except clause to upload/read the dataframe in a local and non-local computer.\n",
    "try:\n",
    "    dropoff_location = pd.read_csv('/datasets/project_sql_result_04.csv')\n",
    "except:\n",
    "    dropoff_location = pd.read_csv('moved_project_sql_result_04 (2).csv')\n",
    "\n",
    "#Use the head() method in order to print the first five rows of the dropoff_location dataframe.\n",
    "dropoff_location.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a try and except clause in order to read/upload the dataframe in a local and nonlocal\n",
    "#computer.\n",
    "try:\n",
    "    weather_conditions = pd.read_csv('/datasets/project_sql_result_07.csv')\n",
    "except:\n",
    "    weather_conditions = pd.read_csv('moved_project_sql_result_07 (3).csv')\n",
    "\n",
    "#Use the head() method to print the first five rows of the dataframe.\n",
    "weather_conditions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"; style=\"border-left: 7px solid gold\">\n",
    "<b>⚠️ Reviewer's comment, v. 1</b> \n",
    "\n",
    "Just clarifying that this is not really a weather conditions dataframe, but the trip duration (plus in what conditions the trip was done)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the info() method on weather_records dataframe in order to check the datatypes in each column.\n",
    "weather_records.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Result:\n",
    "The temperature and description columns have the correct datatypes, but the date_and_time column needs to be converted from object data type to datetime64[ns] datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the pd.to_datetime() function in order to convert the date_and_time column of the \n",
    "#weather_records dataframe from object to datetime64[ns].\n",
    "weather_records['date_and_time'] = pd.to_datetime(weather_records['date_and_time'], format='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the info() method in order to recheck the datatypes in the columns of the weather_records dataframe.\n",
    "weather_records.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Result:\n",
    "The date_and_time column of the weather_records dataframe was converted to datetime64[ns] successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the info() method in order to check the data types in the columns of the company_trips dataframe.\n",
    "company_trips.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Result:\n",
    "The columns in the company_name dataframe have the appropriate data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the info() method in order to check the data types in the columns of the dropoff_location dataframe.\n",
    "dropoff_location.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Result:\n",
    "The columns in the dropoff_location dataframe have the appropriate data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the info() method on weather_conditions dataframe in order to check the datatypes in each column.\n",
    "weather_conditions.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Result:\n",
    "The temperature and description columns have the correct datatypes, but the start_ts column needs to be converted from object data type to datetime64[ns] datatype."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"; style=\"border-left: 7px solid green\">\n",
    "<b>✅ Reviewer's comment, v. 1</b> \n",
    "    \n",
    "You are absolutely right - `start_ts` column needs to have the datatime type.</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the pd.to_datetime() function in order to convert the start_ts column of the weather_conditions \n",
    "#dataframe from object to datetime64[ns].\n",
    "weather_conditions['start_ts'] = pd.to_datetime(weather_conditions['start_ts'], format='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the info() method in order to recheck the datatypes in the columns of the weather_conditions dataframe.\n",
    "weather_conditions.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Result:\n",
    "The start_ts column of the weather_conditions dataframe was converted to datetime64[ns] successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking For Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the duplicated() and sum() methods in order to count duplicate values in the weather_records dataframe.\n",
    "weather_records.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Result:\n",
    "There are no duplicate values noted in the weather_records dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the duplicated() and sum() methods in order to count duplicate values in the company_trips dataframe.\n",
    "company_trips.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Result:\n",
    "There are no duplicate values noted in the company_trips dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the duplicated() and sum() methods in order to count duplicate values in the dropoff_location dataframe.\n",
    "dropoff_location.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Result:\n",
    "There are no duplicate values noted in the dropoff_location dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the duplicated() and sum() methods in order to count duplicate values in the weather_conditions dataframe.\n",
    "weather_conditions.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Result:\n",
    "There are 197 duplicate values noted in the weather_conditions dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"; style=\"border-left: 7px solid gold\">\n",
    "<b>⚠️ Reviewer's comment, v. 1</b> \n",
    "\n",
    "\n",
    "\n",
    "I'm not sure we are having real duplicates here, as there can be trips that started in the same hour and lasting for the same number of seconds. So we probably should not delete them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the drop_duplicates method in order to drop the duplicate values from the weather_conditions dataframe.\n",
    "weather_conditions.drop_duplicates(inplace=True)\n",
    "\n",
    "#Use the duplicated() and the sum() methods in order to count \n",
    "#the number of duplicate values in the weather_conditions column once again.\n",
    "weather_conditions.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Result:\n",
    "There are no more duplicate values noted in the weather_conditions dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"; style=\"border-left: 7px solid green\">\n",
    "<b>✅ Reviewer's comment, v. 1</b> \n",
    "    \n",
    "Very good that you checked for the duplicates.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking For Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the isna() and sum() methods in order to count the number of missing values in each column of the \n",
    "#weather_records dataframe.\n",
    "weather_records.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Result:\n",
    "There are no missing values noted in the weather_records dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the isna() and sum() methods in order to count the number of missing values in each column of the company_trips \n",
    "#dataframe.\n",
    "company_trips.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Result:\n",
    "There are no missing values noted in the company_trips dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the isna() and sum() methods in order to count the number of missing values in each column of \n",
    "#the dropoff_location dataframe.\n",
    "dropoff_location.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Result:\n",
    "There are no missing values noted in the dropoff_location dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the isna() and sum() methods in order to count the number of missing values in each column \n",
    "#of the weather_conditions dataframe.\n",
    "weather_conditions.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Result:\n",
    "There are no missing values noted in the weather_conditions dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"; style=\"border-left: 7px solid green\">\n",
    "<b>✅ Reviewer's comment, v. 1</b> \n",
    "    \n",
    "You've done an excellent data checks and preparation for the further analysis.</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar Chart For Number of Rides Per Taxi Company (Task from Step 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the groupby() and sum() methods in order to sum up the stacked values in the trips_amount\n",
    "#column of the company_name dataframe.\n",
    "c_sum = company_trips.groupby('company_name')['trips_amount'].sum()\n",
    "\n",
    "#Use the reset_index() method in on the c_sum dataframe in order to reset the index. Use the rename()\n",
    "#method in order to rename the trips_amount column to trips_amount_sum()\n",
    "c_sum = c_sum.reset_index().rename(columns={'trips_amount': 'trips_amount_sum'})\n",
    "\n",
    "#Use the sort_values method in order to sort the column by the values in the trips_amount_sum\n",
    "#column.\n",
    "c_sum = c_sum.sort_values(by='trips_amount_sum', ascending=False)\n",
    "\n",
    "#Use the head() method in order to print the first five rows of trips_amount_sum.\n",
    "c_sum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an index that contain the appropriate company names that will be used for the bar chart that contains\n",
    "#the top 10 neighborhoods in terms of drop-offs.\n",
    "company = ['Flash Cab', \n",
    "           'Taxi Affiliation Services', \n",
    "           'Medallion Leasin',\n",
    "           'Yellow Cab',\n",
    "           'Taxi Affiliation Service Yellow',\n",
    "           'Chicago Carriage Cab Corp',\n",
    "           'City Service', \n",
    "           'Sun Taxi',\n",
    "           'Star North Management LLC',\n",
    "           'Blue Ribbon Taxi Association Inc.']\n",
    "\n",
    "#Use the isin() method on the company_name column of the c_sum dataframe in order to check for any values\n",
    "#from the company variable in the column of the dataframe.\n",
    "c_trip = c_sum['company_name'].isin(company)\n",
    "\n",
    "#Use conditional filtering in order to create the c_trips dataframe.\n",
    "c_trips = c_sum[c_trip]\n",
    "\n",
    "#Use the head() method to print the first 5 rows of the c_trips dataframe.\n",
    "c_trips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Use matplotlib and seaborn in order to create a bar chart for the number of\n",
    "#rides per taxi company.\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(\n",
    "    data=c_trips,\n",
    "    x=\"company_name\",\n",
    "    y=\"trips_amount_sum\",\n",
    "    hue=\"company_name\",\n",
    "    dodge=False\n",
    ") \n",
    "\n",
    "plt.title(\"Number of Rides Per Taxi Company\")\n",
    "plt.xlabel(\"Company Name\")\n",
    "plt.ylabel(\"Trips Amount Sum\")\n",
    "plt.legend(title=\"Company Name\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion/ Answer:\n",
    "I chose 10 companies for this analysis which are:\n",
    "1) Flash Cab- with an average trips amount sum of 19,558.\n",
    "2) Taxi Affiliation Services- with an average trips amount sum of 11,422.\n",
    "3) Medallion Leasin- with an average trips amount sum of 10,367.\n",
    "4) Yellow Cab- with an average trips amount sum of 9,888.\n",
    "5) Tax Affiliation Service Yellow- with an average trips amount sum of 9,299.\n",
    "6) Chicago Carriage Cab Corp- with an average trips amount sum of 9,181.\n",
    "7) City Service- with an average trips amount sum of 8,448.\n",
    "8) Sun Taxi- with an average trips amount sum of 7,701.\n",
    "9) Star North Management LLC- with an average trips amount sum of 7,455.\n",
    "10) Blue Ribbon Taxi Association Inc.- with an average trips amount sum of 5,953."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"; style=\"border-left: 7px solid green\">\n",
    "<b>✅ Reviewer's comment, v. 1</b> \n",
    "    \n",
    "You draw a very nice bar chart with all the necessary elements - a title, axis names. However, the legend is redundant here, because each bar is titled.\n",
    "    \n",
    " </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify the top 10 neighborhoods in terms of drop-offs (Task from Step 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group the dataframe by dropoff_location_name.  Use the sum() method in order to \n",
    "#sum the values in 'average_trips' for each dropoff location.\n",
    "top_n_sum = dropoff_location.groupby('dropoff_location_name')['average_trips'].sum()\n",
    "\n",
    "#Use the reset_index() and rename() methods to rename the index and rename the columns.\n",
    "top_n_sum = top_n_sum.reset_index().rename(columns={'average_trips': 'average_trips_sum'})\n",
    "\n",
    "#Use the sort_values() method to sort the values by the average_trips_sum column in descending order.\n",
    "top_n_sum = top_n_sum.sort_values(by='average_trips_sum', ascending=False)\n",
    "top_n_sum.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list of the dropoff location names that will be used in the bar chart.\n",
    "location = ['Loop', \n",
    "            'River North', \n",
    "            'Streeterville', \n",
    "            'West Loop',\n",
    "            \"O'Hare\", \n",
    "            'Lake View', \n",
    "            'Grant Park', \n",
    "            'Museum Campus', \n",
    "            'Gold Coast', \n",
    "            'Sheffield & DePaul']\n",
    "\n",
    "#Use the isin() method in order top check with dropoff locations are in the location list and \n",
    "#return a boolean series of the results.\n",
    "top_10 = top_n_sum['dropoff_location_name'].isin(location)\n",
    "\n",
    "#Use conditional filtering in order to filter out the columns in the top_n_sum\n",
    "#column that are present in the top_10 variable.\n",
    "top_10_df = top_n_sum[top_10]\n",
    "\n",
    "#Use the head() method in order to print the first five rows of the dataframe.\n",
    "top_10_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Use matplotlib and seaborn in order to create a bar chart of the 'Top 10 Neighborhoods In\n",
    "#Terms of Dropoffs\".\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "sns.barplot(\n",
    "    data=top_10_df,\n",
    "    x=\"dropoff_location_name\",\n",
    "    y=\"average_trips_sum\",\n",
    "    hue=\"dropoff_location_name\",\n",
    "    dodge=False)\n",
    "\n",
    "plt.title(\"Top 10 Neighborhoods In Terms Of Dropoffs\")\n",
    "plt.xlabel(\"Dropoff Location Name\")\n",
    "plt.ylabel(\"Average Trips Sum\")\n",
    "plt.legend(title=\"Dropoff Location Name\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion/ Answer:\n",
    "The top 10 neighborhoods in terms of dropoffs are listed:  \n",
    "1) Loop- with an average trips sum of 10,727.47.\n",
    "2) River North- with an average trips sum of 9,523.\n",
    "3) Streeterville- with an average trips sum of 6,664.667.\n",
    "4) West Loop- with an average trips sum of 5,163.667.\n",
    "5) O'Hare- with an average trips sum of 2,549.9.\n",
    "6) Lake View- with an average trips sum of 2,420.\n",
    "7) Grant Park- with an average trips sum of 2,068.533. \n",
    "8) Museum Campus- with an average trips sum of 1,510.\n",
    "9) Gold Coast- with an average trips sum of 1,364.233.\n",
    "10) Sheffield & DePaul- with an average trips sum of 1,259.767."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"; style=\"border-left: 7px solid green\">\n",
    "<b>✅ Reviewer's comment, v. 1</b> \n",
    "    \n",
    "OK, good.</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Question (Task from Step 5)\n",
    "\"The average duration of rides from the Loop to O'Hare International Airport changes on rainy Saturdays.\"\n",
    "Filter bad.\n",
    "1 population is good and 1 population is bad and you test the hypothesis.\n",
    "calculated the variance, calculate standard deviation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Null Hypothesis:\n",
    "The average duration of rides from the Loop to O'Hare International Airport does not change on rainy Saturdays.\n",
    "\n",
    "Alternate Hypothesis:\n",
    "The average duration of rides from the Loop to O'Hare International Airport changes on rainy Saturdays.\n",
    "\n",
    "Rationale For Alternative and Null Hypotheses:\n",
    "* I chose to make the proposed hypothesis in the project the alternative hypothesis because it indicates that the two samples are not equal(duration of rides changes on rainy Saturdays).  I formulated the null hypothesis by stating the opposite of the proposed hypothesis, which would state the two samples are equal(duration of rides does not change on  rainy Saturdays)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"; style=\"border-left: 7px solid green\">\n",
    "<b>✅ Reviewer's comment, v. 1</b> \n",
    "    \n",
    "The hypotheses are correct.</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using conditional filtering, create a dataframe that only lists rows that \n",
    "#have 'Good' as an entry for the weather_condiitons column.\n",
    "good = weather_conditions[weather_conditions['weather_conditions'] == 'Good']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using conditional filtering, create a dataframe that only lists rows that have \n",
    "#'Bad' as an entry for the weather_condiitons column.\n",
    "bad = weather_conditions[weather_conditions['weather_conditions'] == 'Bad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use np.var() in order to calculate the variabnce in the duration_seconds column of the bad dataframe.\n",
    "bad_var = np.var(bad['duration_seconds'])\n",
    "\n",
    "print(f\"Variance of duration_seconds column of the 'bad' dataframe: {bad_var:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use np.var() in order to calculate the variance in the duration_seconds column of the good dataframe.\n",
    "good_var = np.var(good['duration_seconds'])\n",
    "\n",
    "print(f\"Variance of duration_seconds column of the 'good' dataframe: {good_var:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall Conclusion:  The variances of the duration_seconds column of the bad dataframe and the good dataframe are not equal.  The variance of the durations_seconds column of the good dataframe is greater than the variance of the duration_seconds column of the bad dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use np.std() in order to calculate the standard deviation of the duration_seconds column of the bad\n",
    "#dataframe.\n",
    "bad_std = np.std(bad['duration_seconds']) \n",
    "\n",
    "print(f\"Standard deviation of the duration_seconds column of the 'bad' dataframe: {bad_std:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use np.std() in order to calculate the standard deviation of the duration_seconds column of the \n",
    "#good dataframe.\n",
    "good_std = np.std(good['duration_seconds'])\n",
    "print(f\"Standard deviation of the duration_seconds column of the 'good' dataframe: {good_std:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall Conclusion:\n",
    "The standard deviation of the duration_seconds column of the good dataframe is greater than the standard deviation of the duration_seconds column of the bad dataframe.  The two standard deviations are not equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rationale for ttest:\n",
    "* I chose to use the ttest_ind() function to test the hypothesis because it measures the means of two independent samples of scores (duration of seconds when weather is good and duration of seconds when weather is bad.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a variable called alpha with the value 0.05.  This variable will represent the 5 percent probability \n",
    "#that will be used for the ttest.\n",
    "alpha = 0.05\n",
    "\n",
    "#Use the ttest_ind() function from the scipy library with the duration_seconds column of \n",
    "#the bad dataframe as the first arguement and the duration_seconds column of the good dataframe as the second arguement.  \n",
    "#Set the equal_var parameter to False because the two variances are not equal.\n",
    "results = st.ttest_ind(bad['duration_seconds'], good['duration_seconds'], equal_var=False)\n",
    "\n",
    "#Create an if-else statement that prints the string \"We reject null hypothesis\" if results.pvalue is less \n",
    "#than alpha or prints the string \"We cannot reject null hypothesis\" if the else statement is valid.\n",
    "print('pvalue:', results.pvalue)\n",
    "if results.pvalue < alpha:\n",
    "    print(\"We reject null hypothesis\")\n",
    "else:\n",
    "    print(\"We cannot reject null hypothesis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion/ Answer:\n",
    "According to the ttest, the hypothesis that states: \"The average duration of rides from the Loop to O'Hare International Airport changes on rainy Saturdays.\", is correct because the null hypothesis has been rejected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"; style=\"border-left: 7px solid green\">\n",
    "<b>✅ Reviewer's comment, v. 1</b> \n",
    "    \n",
    "You've created the samples correctly and used a suitable statistical test. Indeed, the test shows that the null hypothesis can be rejected.  </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"; style=\"border-left: 7px solid gold\">\n",
    "<b>⚠️ Reviewer's comment, v. 1</b> \n",
    "\n",
    "We do not say that an alternative hypothesis is correct. We usually say that it can be accepted if we rejected the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Overall Conclusion:\n",
    "1) From the list of taxi companies that I chose to display in the first bar chart of the exploratory data analysis portion of the project, the company with the most amount of trips per company was Flash Cab, which had 19,558 trips.  The company with the least amount of trips per company was Blue Ribbon Taxi Association Inc., with 5,953 trips.\n",
    "\n",
    "2) The top 10 neighborhoods in terms of dropoffs are:\n",
    "   1) Loop\n",
    "   2) River North\n",
    "   3) Streeterville\n",
    "   4) West Loop\n",
    "   5) O'Hare\n",
    "   6) Lake View\n",
    "   7) Grant Park\n",
    "   8) Museum Campus\n",
    "   9) Gold Coast\n",
    "   10) Sheffield & DePaul\n",
    "\n",
    "3) The proposed hypothesis stating, \"The average duration of rides from the Loop to O'Hare International Airport changes on rainy Saturdays.\", was proven correct according to the ttest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"; style=\"border-left: 7px solid green\">\n",
    "<b>✅ Reviewer's comment, v. 1</b> \n",
    "    \n",
    "Excellent that you did not forget to write the conclusion.</div>\n"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 1309,
    "start_time": "2024-07-04T03:52:37.610Z"
   },
   {
    "duration": 7,
    "start_time": "2024-07-04T03:52:38.923Z"
   },
   {
    "duration": 415,
    "start_time": "2024-07-04T03:52:38.934Z"
   },
   {
    "duration": 126,
    "start_time": "2024-07-04T03:52:39.353Z"
   },
   {
    "duration": 5,
    "start_time": "2024-07-04T03:52:39.482Z"
   },
   {
    "duration": 8,
    "start_time": "2024-07-04T03:52:39.491Z"
   },
   {
    "duration": 50,
    "start_time": "2024-07-04T03:52:39.502Z"
   },
   {
    "duration": 18,
    "start_time": "2024-07-04T03:52:39.555Z"
   },
   {
    "duration": 12,
    "start_time": "2024-07-04T03:52:39.576Z"
   },
   {
    "duration": 51,
    "start_time": "2024-07-04T03:52:39.595Z"
   },
   {
    "duration": 17,
    "start_time": "2024-07-04T03:52:39.650Z"
   },
   {
    "duration": 22,
    "start_time": "2024-07-04T03:52:39.671Z"
   },
   {
    "duration": 40,
    "start_time": "2024-07-04T03:52:39.697Z"
   },
   {
    "duration": 5,
    "start_time": "2024-07-04T03:52:39.741Z"
   },
   {
    "duration": 14,
    "start_time": "2024-07-04T03:52:39.749Z"
   },
   {
    "duration": 12,
    "start_time": "2024-07-04T03:52:39.766Z"
   },
   {
    "duration": 57,
    "start_time": "2024-07-04T03:52:39.781Z"
   },
   {
    "duration": 14,
    "start_time": "2024-07-04T03:52:39.841Z"
   },
   {
    "duration": 6,
    "start_time": "2024-07-04T03:52:39.858Z"
   },
   {
    "duration": 15,
    "start_time": "2024-07-04T03:52:39.867Z"
   },
   {
    "duration": 8,
    "start_time": "2024-07-04T03:52:39.884Z"
   },
   {
    "duration": 8,
    "start_time": "2024-07-04T03:52:39.932Z"
   },
   {
    "duration": 8,
    "start_time": "2024-07-04T03:52:39.942Z"
   },
   {
    "duration": 9,
    "start_time": "2024-07-04T03:52:39.953Z"
   },
   {
    "duration": 11,
    "start_time": "2024-07-04T03:52:39.964Z"
   },
   {
    "duration": 57,
    "start_time": "2024-07-04T03:52:39.979Z"
   },
   {
    "duration": 8,
    "start_time": "2024-07-04T03:52:40.039Z"
   },
   {
    "duration": 8,
    "start_time": "2024-07-04T03:52:40.055Z"
   },
   {
    "duration": 8,
    "start_time": "2024-07-04T03:52:40.066Z"
   },
   {
    "duration": 64,
    "start_time": "2024-07-04T03:52:40.077Z"
   },
   {
    "duration": 17,
    "start_time": "2024-07-04T03:52:40.145Z"
   },
   {
    "duration": 612,
    "start_time": "2024-07-04T03:52:40.166Z"
   },
   {
    "duration": 34,
    "start_time": "2024-07-04T03:52:40.781Z"
   },
   {
    "duration": 16,
    "start_time": "2024-07-04T03:52:40.834Z"
   },
   {
    "duration": 161,
    "start_time": "2024-07-04T03:52:40.854Z"
   },
   {
    "duration": 14,
    "start_time": "2024-07-04T03:52:41.018Z"
   },
   {
    "duration": 6,
    "start_time": "2024-07-04T03:52:41.036Z"
   },
   {
    "duration": 7,
    "start_time": "2024-07-04T03:52:41.045Z"
   },
   {
    "duration": 7,
    "start_time": "2024-07-04T03:52:41.055Z"
   },
   {
    "duration": 7,
    "start_time": "2024-07-04T03:52:41.065Z"
   },
   {
    "duration": 7,
    "start_time": "2024-07-04T03:52:41.074Z"
   },
   {
    "duration": 7,
    "start_time": "2024-07-04T03:52:41.132Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
